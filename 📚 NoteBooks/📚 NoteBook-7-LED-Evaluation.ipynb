{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe73633",
   "metadata": {},
   "source": [
    "# üìö NoteBook 8 LED Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b819545",
   "metadata": {},
   "source": [
    "# üöÄ PROJECT PLAN\n",
    "\n",
    "MKEM Implementation ‚Äì Transformer-Based Abstractive Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d971c",
   "metadata": {},
   "source": [
    "# üéØ Problem Statement Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a7e313",
   "metadata": {},
   "source": [
    "# üîç Objective:\n",
    "    \n",
    "To build and compare transformer-based summarization models (T5, BART, Pegasus,BARTScore,ProphetNet,BigBird,LED,mTS,FLAN-T5,GPT 3.5 Turbo) and then enhance them using MKEM (Multi-Knowledge-Enhanced Model) on curated English news datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70ae6c",
   "metadata": {},
   "source": [
    "# üìå Phase-1 Objective\n",
    "\n",
    "‚úÖ Implement the following 3 summarization models:\n",
    "\n",
    "PEGASUS (Google)---NoteBook(2)\n",
    "\n",
    "BART (Facebook)---NoteBook(3)\n",
    "\n",
    "T5 (Google)---NoteBook(1)\n",
    "\n",
    "Final Comparison + MKEM---NoteBook(4)\n",
    "\n",
    "NewsSum(Indian Newspaper)---NoteBook(5)\n",
    "\n",
    "BARTScore---NoteBook(6)\n",
    "\n",
    "ProphetNet---NoteBook(7)\n",
    "\n",
    "BigBird-Pegasus---NoteBook(8)\n",
    "\n",
    "LED(Longformer)---NoteBook(9)\n",
    "\n",
    "allenai/PRIMERA ---NoteBook(10)\n",
    "\n",
    "FLAN-T5---NoteBook(11)\n",
    "\n",
    "GPT-3.5 Turbo---NoteBook(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da9340",
   "metadata": {},
   "source": [
    "# ‚úÖ Evaluate on 3 benchmark datasets:\n",
    "    \n",
    "CNN/DailyMail\n",
    "\n",
    "Newssum (IndianNewsPaper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a8d278",
   "metadata": {},
   "source": [
    "# ‚úÖ Evaluation Metrics:\n",
    "    \n",
    "ROUGE-1\n",
    "\n",
    "ROUGE-2\n",
    "\n",
    "ROUGE-L\n",
    "\n",
    "BERTScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2a9e3",
   "metadata": {},
   "source": [
    "# üìä Final Output (Per Model √ó Dataset):\n",
    "    \n",
    "You must submit structured results:\n",
    "\n",
    "Dataset name\n",
    "\n",
    "Model used\n",
    "\n",
    "ROUGE-1, ROUGE-2, ROUGE-L, BERTScore\n",
    "\n",
    "Inference Time\n",
    "\n",
    "GPU used\n",
    "\n",
    "Short analysis/observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6423de8",
   "metadata": {},
   "source": [
    "# **1.üöÄ LED on CNN Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac82535",
   "metadata": {},
   "source": [
    "**‚úèÔ∏èStep 1: Install & Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff459322",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate sentencepiece evaluate bert-score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "463775f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMIM IMTIAZ\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "import time\n",
    "import evaluate\n",
    "import bert_score\n",
    "\n",
    "# Device setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"‚úÖ Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc012d",
   "metadata": {},
   "source": [
    "**‚úèÔ∏è Step 2: Load Model & Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d2972d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c12a66b89cc44b1a3078355dda6d694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMIM IMTIAZ\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SAMIM IMTIAZ\\.cache\\huggingface\\hub\\models--allenai--led-base-16384. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392a1d82185e439f9bc15322d45b70b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659703beeb7f4b6692ce146f121847b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec4a75ea0564c5fbaa104bb266ca455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237b7e45fc6b43078aabb8ef6188a918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ced809f396408b862a680ca0805acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/648M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753ae73162d14ad68d72c051559ca71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LED model and tokenizer loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ec1164149244d7a342a14c365fac0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/648M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‚úèÔ∏è Step 2: Load Model & Tokenizer for LED\n",
    "model_name = \"allenai/led-base-16384\"\n",
    "\n",
    "tokenizer_led = AutoTokenizer.from_pretrained(model_name)\n",
    "model_led = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(\"‚úÖ LED model and tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d693ff",
   "metadata": {},
   "source": [
    "**‚úèÔ∏è Step 3: Load CNN Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c4536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CNN Dataset loaded with 5 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets ¬£20M f...</td>\n",
       "      <td>42c027e4ff9730fbb3de84c1af0d2c506e41c3e4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Editor's note: In our Behind the Scenes series...</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
       "      <td>ee8871b15c50d0db17b0179a6d2beab35065f1e9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...</td>\n",
       "      <td>NEW: \"I thought I was going to die,\" driver sa...</td>\n",
       "      <td>06352019a19ae31e527f37f7571c6dd7f0c5da37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WASHINGTON (CNN) -- Doctors removed five small...</td>\n",
       "      <td>Five small polyps found during procedure; \"non...</td>\n",
       "      <td>24521a2abb2e1f5e34e6824e0f9e56904a2b0e88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(CNN)  -- The National Football League has ind...</td>\n",
       "      <td>NEW: NFL chief, Atlanta Falcons owner critical...</td>\n",
       "      <td>7fe70cc8b12fab2d0a258fababf7d9c6b5e1262a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  LONDON, England (Reuters) -- Harry Potter star...   \n",
       "1  Editor's note: In our Behind the Scenes series...   \n",
       "2  MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...   \n",
       "3  WASHINGTON (CNN) -- Doctors removed five small...   \n",
       "4  (CNN)  -- The National Football League has ind...   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Harry Potter star Daniel Radcliffe gets ¬£20M f...   \n",
       "1  Mentally ill inmates in Miami are housed on th...   \n",
       "2  NEW: \"I thought I was going to die,\" driver sa...   \n",
       "3  Five small polyps found during procedure; \"non...   \n",
       "4  NEW: NFL chief, Atlanta Falcons owner critical...   \n",
       "\n",
       "                                         id  \n",
       "0  42c027e4ff9730fbb3de84c1af0d2c506e41c3e4  \n",
       "1  ee8871b15c50d0db17b0179a6d2beab35065f1e9  \n",
       "2  06352019a19ae31e527f37f7571c6dd7f0c5da37  \n",
       "3  24521a2abb2e1f5e34e6824e0f9e56904a2b0e88  \n",
       "4  7fe70cc8b12fab2d0a258fababf7d9c6b5e1262a  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load cleaned CNN dataset (from Notebook 1)\n",
    "df_cnn = pd.read_csv(\"cnn_dailymail.csv\")\n",
    "\n",
    "# Drop missing or empty articles/highlights\n",
    "df_cnn = df_cnn.dropna(subset=[\"article\", \"highlights\"])\n",
    "df_cnn = df_cnn[df_cnn[\"article\"].str.strip().astype(bool)]\n",
    "\n",
    "# Optional: Limit to small sample for quick testing\n",
    "df_cnn = df_cnn[:5]\n",
    "\n",
    "print(f\"‚úÖ CNN Dataset loaded with {len(df_cnn)} rows.\")\n",
    "df_cnn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2756e3",
   "metadata": {},
   "source": [
    "**‚úèÔ∏è Step 4: Define Summarization Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8154260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with_led(text):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer_led(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=4096  # LED supports long input sequences\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary_ids = model_led.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=150,\n",
    "        min_length=40,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4\n",
    "    )\n",
    "    \n",
    "    # Decode and return\n",
    "    return tokenizer_led.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649ebc6f",
   "metadata": {},
   "source": [
    "**‚úèÔ∏è Step 5: Generate Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5add0b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "`cache.key_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].keys` instead.\n",
      "`cache.value_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].values` instead.\n",
      "Input ids are automatically padded from 888 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 919 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1196 to 2048 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LED summarization completed in 108.81 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate summaries for CNN articles\n",
    "led_cnn_preds = [summarize_with_led(article) for article in df_cnn[\"article\"]]\n",
    "\n",
    "# Get references (human-written summaries)\n",
    "led_cnn_refs = df_cnn[\"highlights\"].tolist()\n",
    "\n",
    "# Measure inference time\n",
    "inference_time = round(time.time() - start_time, 2)\n",
    "print(f\"‚úÖ LED summarization completed in {inference_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67524ed",
   "metadata": {},
   "source": [
    "**‚úèÔ∏è Step 6: Evaluate with ROUGE & BERTScore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a77dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\SAMIM IMTIAZ\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LED CNN evaluation saved to LED_CNN_Evaluation.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Model</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>BERTScore</th>\n",
       "      <th>Inference Time (s)</th>\n",
       "      <th>GPU Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>LED</td>\n",
       "      <td>0.28072</td>\n",
       "      <td>0.121688</td>\n",
       "      <td>0.190097</td>\n",
       "      <td>0.8515</td>\n",
       "      <td>108.81</td>\n",
       "      <td>CPU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset Model  ROUGE-1   ROUGE-2   ROUGE-L  BERTScore  Inference Time (s)  \\\n",
       "0     CNN   LED  0.28072  0.121688  0.190097     0.8515              108.81   \n",
       "\n",
       "  GPU Used  \n",
       "0      CPU  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "\n",
    "# Load metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# ‚úÖ Compute ROUGE\n",
    "rouge_scores = rouge.compute(predictions=led_cnn_preds, references=led_cnn_refs)\n",
    "\n",
    "# ‚úÖ Compute BERTScore\n",
    "bert_scores = bertscore.compute(predictions=led_cnn_preds, references=led_cnn_refs, lang=\"en\")\n",
    "\n",
    "# ‚úÖ Prepare results dictionary\n",
    "led_cnn_results = {\n",
    "    \"Dataset\": [\"CNN\"],\n",
    "    \"Model\": [\"LED\"],\n",
    "    \"ROUGE-1\": [rouge_scores[\"rouge1\"]],\n",
    "    \"ROUGE-2\": [rouge_scores[\"rouge2\"]],\n",
    "    \"ROUGE-L\": [rouge_scores[\"rougeL\"]],\n",
    "    \"BERTScore\": [round(sum(bert_scores[\"f1\"]) / len(bert_scores[\"f1\"]), 4)],\n",
    "    \"Inference Time (s)\": [inference_time],\n",
    "    \"GPU Used\": [torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"]\n",
    "}\n",
    "\n",
    "# ‚úÖ Save to CSV\n",
    "import pandas as pd\n",
    "led_cnn_df = pd.DataFrame(led_cnn_results)\n",
    "led_cnn_df.to_csv(\"LED_CNN_Evaluation.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ LED CNN evaluation saved to LED_CNN_Evaluation.csv\")\n",
    "led_cnn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ab1a8",
   "metadata": {},
   "source": [
    "# üíæ Save the Scores to .CSV Files\n",
    "\n",
    "So that we can use to comapair models in different NoteBooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35baf2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LED CNN evaluation saved to LED_CNN_Evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "led_cnn_df = pd.DataFrame(led_cnn_results)\n",
    "led_cnn_df.to_csv(\"LED_CNN_Evaluation.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ LED CNN evaluation saved to LED_CNN_Evaluation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6eff48",
   "metadata": {},
   "source": [
    "# 2.üöÄ LED on NewsSum Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ea850",
   "metadata": {},
   "source": [
    "**‚úèÔ∏è Step 1: Load NewsSum Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db62cd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Article</th>\n",
       "      <th>Category</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elephant death brings to fore man-animal confl...</td>\n",
       "      <td>The death of a pregnant elephant in the buffer...</td>\n",
       "      <td>Local News</td>\n",
       "      <td>Thousands of farmers in Kerala have either aba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cases filed after two √¢‚Ç¨Àúcommit suicide√¢‚Ç¨‚Ñ¢ in ...</td>\n",
       "      <td>Two suicides were reported from Vadodara and D...</td>\n",
       "      <td>Crime and Justice</td>\n",
       "      <td>In the first incident, a 30-year-old woman all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Woman alleges father tied to MP hospital bed o...</td>\n",
       "      <td>A day after a woman alleged that her father ha...</td>\n",
       "      <td>Health and Wellness</td>\n",
       "      <td>The hospital denied the allegation, saying the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sena member, author, app designer √¢‚Ç¨‚Äú the many...</td>\n",
       "      <td>Assistant police inspector Sachin Vaze, who wa...</td>\n",
       "      <td>Defense</td>\n",
       "      <td>On Saturday, Vaze along with police constables...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Manager, owner of resort where Gujarat Congres...</td>\n",
       "      <td>The manager and owner of a resort in Rajkot, w...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>The resort is reportedly owned by Indranil Raj...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0  Elephant death brings to fore man-animal confl...   \n",
       "1  Cases filed after two √¢‚Ç¨Àúcommit suicide√¢‚Ç¨‚Ñ¢ in ...   \n",
       "2  Woman alleges father tied to MP hospital bed o...   \n",
       "3  Sena member, author, app designer √¢‚Ç¨‚Äú the many...   \n",
       "4  Manager, owner of resort where Gujarat Congres...   \n",
       "\n",
       "                                             Article             Category  \\\n",
       "0  The death of a pregnant elephant in the buffer...           Local News   \n",
       "1  Two suicides were reported from Vadodara and D...    Crime and Justice   \n",
       "2  A day after a woman alleged that her father ha...  Health and Wellness   \n",
       "3  Assistant police inspector Sachin Vaze, who wa...              Defense   \n",
       "4  The manager and owner of a resort in Rajkot, w...             Politics   \n",
       "\n",
       "                                             Summary  \n",
       "0  Thousands of farmers in Kerala have either aba...  \n",
       "1  In the first incident, a 30-year-old woman all...  \n",
       "2  The hospital denied the allegation, saying the...  \n",
       "3  On Saturday, Vaze along with police constables...  \n",
       "4  The resort is reportedly owned by Indranil Raj...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load cleaned NewsSum dataset\n",
    "df_newsum = pd.read_csv(\"newsum_cleaned.csv\")\n",
    "\n",
    "# Drop any missing rows\n",
    "df_newsum = df_newsum.dropna(subset=[\"Article\", \"Summary\"])\n",
    "df_newsum = df_newsum[df_newsum[\"Article\"].str.strip().astype(bool)]\n",
    "\n",
    "df_newsum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d1e13",
   "metadata": {},
   "source": [
    "**‚úèÔ∏èStep 2: Generate Summaries with LED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ece437",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1230 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1761 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1279 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 995 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 937 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 512 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1203 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 572 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 495 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1616 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 555 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1266 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1002 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1073 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 918 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 2916 to 3072 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 751 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1704 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 448 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 3395 to 4096 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 880 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1627 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 865 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 528 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1103 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1075 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1092 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 450 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1445 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 386 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 948 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 673 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1025 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1026 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 716 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 720 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1599 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 878 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1592 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 479 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1783 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 2435 to 3072 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 449 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 432 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 263 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1032 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1933 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1461 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1401 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 920 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1036 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 782 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 584 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 456 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1138 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 2324 to 3072 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 321 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 436 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1066 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 830 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 740 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 846 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1110 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 490 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1004 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 850 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 730 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 972 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 797 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1483 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 859 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 354 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1162 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 366 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1165 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 834 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1252 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1177 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 583 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 783 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 500 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1083 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 547 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 441 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 870 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1144 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 744 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 2436 to 3072 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 467 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 416 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1239 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 774 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 711 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 3984 to 4096 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1585 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 438 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1330 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1835 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 980 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 796 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 401 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 945 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 640 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 957 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1197 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 864 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 747 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 927 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 530 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 466 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1909 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 426 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1288 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 433 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 2533 to 3072 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 527 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1056 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1883 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 346 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1080 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1839 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1968 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 893 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 884 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1166 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 538 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1076 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 816 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 421 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 934 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 933 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1017 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1023 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1000 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1131 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1283 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 732 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 881 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1976 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 485 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 403 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 896 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 741 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 833 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 447 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 659 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 989 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 437 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1540 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 790 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1762 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1576 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 601 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 993 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 251 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1189 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 202 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1387 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1224 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 956 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 2102 to 3072 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 2090 to 3072 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 468 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 930 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 3861 to 4096 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1113 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1433 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1952 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1051 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 392 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 481 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 382 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1167 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1136 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1582 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 375 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1013 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1079 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 628 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1125 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1481 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1290 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 399 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 472 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1155 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 502 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 520 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 344 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 488 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 469 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 457 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 971 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1312 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 2095 to 3072 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1091 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 417 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 709 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 926 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 279 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 415 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1456 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1686 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 809 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 362 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 484 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 320 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1755 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 961 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 758 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 429 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 753 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1370 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 832 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1324 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1777 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 402 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 784 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 868 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 665 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 264 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 250 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 349 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 330 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1215 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 452 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 808 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 689 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 687 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 688 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 568 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 599 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1053 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 899 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1109 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1272 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 841 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1034 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 385 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 622 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 442 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 535 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 504 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1011 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 336 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 258 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 959 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 371 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 856 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1459 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 424 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 291 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1897 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1107 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 498 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 492 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 409 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 694 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 405 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 475 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1028 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 510 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 877 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1046 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1850 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 536 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 925 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 297 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 615 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 400 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 843 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 225 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 462 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1105 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 728 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 635 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 353 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1087 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 882 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 963 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 979 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 379 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 620 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 818 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 369 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 3183 to 4096 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1609 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1552 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 869 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 293 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 633 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1221 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 1522 to 2048 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 407 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 242 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 290 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è± Inference Time: 13571.06 seconds for 1003 summaries\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def batch_summarize_with_led(texts, batch_size=2):\n",
    "    preds = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer_led(batch, return_tensors=\"pt\", truncation=True, padding=\"longest\", max_length=4096).to(device)\n",
    "        summary_ids = model_led.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4)\n",
    "        preds.extend([tokenizer_led.decode(g, skip_special_tokens=True) for g in summary_ids])\n",
    "    return preds\n",
    "\n",
    "# ‚úÖ Run in batches\n",
    "start_time = time.time()\n",
    "led_newsum_preds = batch_summarize_with_led(df_newsum[\"Article\"].tolist(), batch_size=2)\n",
    "led_newsum_refs = df_newsum[\"Summary\"].tolist()\n",
    "inference_time = round(time.time() - start_time, 2)\n",
    "\n",
    "print(f\"‚è± Inference Time: {inference_time} seconds for {len(led_newsum_preds)} summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14262967",
   "metadata": {},
   "source": [
    "**‚úèÔ∏èStep 3: Evaluate with ROUGE and BERTScore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2ce39a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\SAMIM IMTIAZ\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "\n",
    "# Load metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# ‚úÖ Compute ROUGE\n",
    "rouge_scores = rouge.compute(predictions=led_newsum_preds, references=led_newsum_refs)\n",
    "\n",
    "# ‚úÖ Compute BERTScore\n",
    "bert_scores = bertscore.compute(predictions=led_newsum_preds, references=led_newsum_refs, lang=\"en\")\n",
    "\n",
    "# ‚úÖ Prepare results dictionary\n",
    "led_newsum_results = {\n",
    "    \"Dataset\": [\"NewsSum\"],\n",
    "    \"Model\": [\"LED\"],\n",
    "    \"ROUGE-1\": [rouge_scores[\"rouge1\"]],\n",
    "    \"ROUGE-2\": [rouge_scores[\"rouge2\"]],\n",
    "    \"ROUGE-L\": [rouge_scores[\"rougeL\"]],\n",
    "    \"BERTScore\": [round(sum(bert_scores[\"f1\"]) / len(bert_scores[\"f1\"]), 4)],\n",
    "    \"Inference Time (s)\": [inference_time],\n",
    "    \"GPU Used\": [torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2141e88",
   "metadata": {},
   "source": [
    "**üíæ Step 4: Save Evaluation Scores to CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d13c1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LED NewsSum evaluation saved to LED_NewsSum_Evaluation.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Model</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>BERTScore</th>\n",
       "      <th>Inference Time (s)</th>\n",
       "      <th>GPU Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsSum</td>\n",
       "      <td>LED</td>\n",
       "      <td>0.330616</td>\n",
       "      <td>0.264168</td>\n",
       "      <td>0.299004</td>\n",
       "      <td>0.8744</td>\n",
       "      <td>13571.06</td>\n",
       "      <td>CPU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dataset Model   ROUGE-1   ROUGE-2   ROUGE-L  BERTScore  Inference Time (s)  \\\n",
       "0  NewsSum   LED  0.330616  0.264168  0.299004     0.8744            13571.06   \n",
       "\n",
       "  GPU Used  \n",
       "0      CPU  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "led_newsum_df = pd.DataFrame(led_newsum_results)\n",
    "led_newsum_df.to_csv(\"LED_NewsSum_Evaluation.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ LED NewsSum evaluation saved to LED_NewsSum_Evaluation.csv\")\n",
    "led_newsum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaa4fcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ led_cnn + led_newsum saved to bigbird_all_scores.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Model</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>BERTScore</th>\n",
       "      <th>Inference Time (s)</th>\n",
       "      <th>GPU Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>CNN</td>\n",
       "      <td>LED</td>\n",
       "      <td>0.280720</td>\n",
       "      <td>0.121688</td>\n",
       "      <td>0.190097</td>\n",
       "      <td>0.8515</td>\n",
       "      <td>108.81</td>\n",
       "      <td>CPU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NewsSum</td>\n",
       "      <td>LED</td>\n",
       "      <td>0.330616</td>\n",
       "      <td>0.264168</td>\n",
       "      <td>0.299004</td>\n",
       "      <td>0.8744</td>\n",
       "      <td>13571.06</td>\n",
       "      <td>CPU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Dataset Model   ROUGE-1   ROUGE-2   ROUGE-L  BERTScore  \\\n",
       "0         0.0      CNN   LED  0.280720  0.121688  0.190097     0.8515   \n",
       "1         NaN  NewsSum   LED  0.330616  0.264168  0.299004     0.8744   \n",
       "\n",
       "   Inference Time (s) GPU Used  \n",
       "0              108.81      CPU  \n",
       "1            13571.06      CPU  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load individual score files\n",
    "led_cnn_df = pd.read_csv(\"LED_CNN_Evaluation.csv\")\n",
    "led_newsum_df = pd.read_csv(\"LED_NewsSum_Evaluation.csv\")\n",
    "\n",
    "# Merge into one DataFrame\n",
    "led_all_scores = pd.concat([led_cnn_df, led_newsum_df], ignore_index=True)\n",
    "led_all_scores.to_csv(\"led_all_scores.csv\", index=False)\n",
    "led_all_scores\n",
    "\n",
    "# Save merged scores\n",
    "led_all_scores.to_csv(\"led_all_scores.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ led_cnn + led_newsum saved to bigbird_all_scores.csv\")\n",
    "led_all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ceea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
